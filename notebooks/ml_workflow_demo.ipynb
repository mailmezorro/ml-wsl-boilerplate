{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2689612d",
   "metadata": {},
   "source": [
    "# ML WSL Boilerplate - Complete Data Science Workflow\n",
    "\n",
    "This notebook demonstrates a complete machine learning workflow using the ML WSL Boilerplate framework.\n",
    "\n",
    "## What we'll cover:\n",
    "\n",
    "1. **Project Setup** - Configuration and imports\n",
    "2. **Data Loading** - Using our custom DataLoader\n",
    "3. **Exploratory Data Analysis** - Understanding the data\n",
    "4. **Feature Engineering** - Preprocessing and feature creation\n",
    "5. **Model Training** - Multiple algorithms with cross-validation\n",
    "6. **Model Evaluation** - Comprehensive metrics and visualization\n",
    "7. **Experiment Tracking** - MLflow integration\n",
    "8. **Model Persistence** - Saving and loading models\n",
    "9. **Results & Next Steps** - Interpretation and recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2844194",
   "metadata": {},
   "source": [
    "## 1. Project Setup\n",
    "\n",
    "Let's start by setting up our environment and loading the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Data science stack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Add our source directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Our custom modules\n",
    "from src.utils.config import get_config\n",
    "from src.utils.logging_config import setup_logging, get_logger\n",
    "from src.data.loader import DataLoader\n",
    "from src.models.trainer import get_trainer\n",
    "\n",
    "# Configure display\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup logging\n",
    "ml_logger = setup_logging(log_level=\"INFO\")\n",
    "logger = get_logger(component=\"notebook\")\n",
    "\n",
    "print(\"ML WSL Boilerplate - Ready for Data Science!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load configuration\n",
    "config = get_config()\n",
    "\n",
    "print(\"Configuration Overview:\")\n",
    "print(f\"Project: {config.get('project.name')}\")\n",
    "print(f\"Model Type: {config.get('model.type')}\")\n",
    "print(f\"Train/Test Split: {config.get('data.train_test_split')}\")\n",
    "print(f\"CV Folds: {config.get('training.cv_folds')}\")\n",
    "print(f\"Random Seed: {config.get('environment.seed')}\")\n",
    "print(f\"MLflow Enabled: {config.get('mlflow.enabled')}\")\n",
    "\n",
    "# Validate configuration\n",
    "if config.validate_config():\n",
    "    print(\"Configuration is valid!\")\n",
    "else:\n",
    "    print(\"Configuration validation failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74924c91",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Generation\n",
    "\n",
    "For this demo, we'll generate a synthetic dataset that simulates a real-world classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cc759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "logger.info(\"Generating synthetic dataset...\")\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.6, 0.4],  # Slightly imbalanced\n",
    "    flip_y=0.01,  # Add some noise\n",
    "    random_state=config.get('environment.seed', 42)\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "feature_names = [f\"feature_{i:02d}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(f\"Dataset created: {df.shape[0]:,} samples, {df.shape[1]-1} features\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df['target'].value_counts(normalize=True).round(3))\n",
    "\n",
    "# Use DataLoader for validation\n",
    "data_loader = DataLoader()\n",
    "validation_results = data_loader.validate_data(df)\n",
    "\n",
    "print(f\"\\nData Validation:\")\n",
    "print(f\"   Shape: {validation_results['shape']}\")\n",
    "print(f\"   Missing values: {sum(validation_results['missing_values'].values())}\")\n",
    "print(f\"   Duplicates: {validation_results['duplicates']}\")\n",
    "print(f\"   Memory usage: {validation_results['memory_usage'] / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffa9ca",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Let's explore our data to understand its characteristics and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bec028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb748b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization setup\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Exploratory Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Target distribution\n",
    "df['target'].value_counts().plot(kind='bar', ax=axes[0,0], color=['skyblue', 'lightcoral'])\n",
    "axes[0,0].set_title('Target Distribution')\n",
    "axes[0,0].set_xlabel('Class')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Feature correlation heatmap (top 10 features)\n",
    "top_features = feature_names[:10] + ['target']\n",
    "corr_matrix = df[top_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, ax=axes[0,1], cbar_kws={\"shrink\": .8})\n",
    "axes[0,1].set_title('Feature Correlation (Top 10)')\n",
    "\n",
    "# 3. Feature distributions by class\n",
    "feature_to_plot = 'feature_00'  # Most important feature\n",
    "for class_val in [0, 1]:\n",
    "    subset = df[df['target'] == class_val][feature_to_plot]\n",
    "    axes[1,0].hist(subset, alpha=0.7, label=f'Class {class_val}', bins=30)\n",
    "axes[1,0].set_title(f'{feature_to_plot} Distribution by Class')\n",
    "axes[1,0].set_xlabel('Feature Value')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Feature importance (correlation with target)\n",
    "feature_importance = df[feature_names].corrwith(df['target']).abs().sort_values(ascending=False)\n",
    "feature_importance.head(10).plot(kind='barh', ax=axes[1,1], color='lightgreen')\n",
    "axes[1,1].set_title('Top 10 Feature Correlations with Target')\n",
    "axes[1,1].set_xlabel('Absolute Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMost correlated features:\")\n",
    "for i, (feature, corr) in enumerate(feature_importance.head(5).items()):\n",
    "    print(f\"   {i+1}. {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a222c",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Data Preprocessing\n",
    "\n",
    "Let's prepare our data for machine learning using our custom preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db24562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df[feature_names].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Clean data using our DataLoader\n",
    "X_clean = data_loader.clean_data(\n",
    "    X,\n",
    "    drop_duplicates=True,\n",
    "    fill_missing=None  # No missing values in synthetic data\n",
    ")\n",
    "\n",
    "print(f\"Data cleaning completed\")\n",
    "print(f\"   Rows before: {len(X)}, after: {len(X_clean)}\")\n",
    "\n",
    "# Split data according to config\n",
    "test_size = 1.0 - config.get('data.train_test_split', 0.8)\n",
    "random_state = config.get('environment.seed', 42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"   Training: {len(X_train):,} samples ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"   Testing:  {len(X_test):,} samples ({len(X_test)/len(X):.1%})\")\n",
    "print(f\"   Train target dist: {y_train.value_counts(normalize=True).round(3).tolist()}\")\n",
    "print(f\"   Test target dist:  {y_test.value_counts(normalize=True).round(3).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c41f3",
   "metadata": {},
   "source": [
    "## 5. Model Training & Cross-Validation\n",
    "\n",
    "Now let's train multiple models and compare their performance using our training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "model_types = ['sklearn', 'xgboost', 'lightgbm']\n",
    "results = {}\n",
    "\n",
    "print(\"Starting model training and evaluation...\\n\")\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type.upper()} model...\")\n",
    "\n",
    "    try:\n",
    "        # Update config for current model\n",
    "        config.set('model.type', model_type)\n",
    "\n",
    "        # Get trainer\n",
    "        trainer = get_trainer(model_type)\n",
    "\n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Cross-validation\n",
    "        cv_results = trainer.cross_validate(\n",
    "            X_train, y_train,\n",
    "            cv_folds=config.get('training.cv_folds', 5),\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "\n",
    "        # Train final model\n",
    "        train_metrics = trainer.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_metrics = trainer.evaluate(X_test, y_test, stage='test')\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Store results\n",
    "        results[model_type] = {\n",
    "            'trainer': trainer,\n",
    "            'cv_results': cv_results,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "        print(f\"   CV Accuracy: {cv_results['cv_mean']:.4f} (±{cv_results['cv_std']:.4f})\")\n",
    "        print(f\"   Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"   Training time: {training_time:.2f}s\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Failed to train {model_type}: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343c577",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation & Comparison\n",
    "\n",
    "Let's compare the performance of our models and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc539c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_type, result in results.items():\n",
    "    row = {\n",
    "        'Model': model_type.upper(),\n",
    "        'CV_Accuracy': result['cv_results']['cv_mean'],\n",
    "        'CV_Std': result['cv_results']['cv_std'],\n",
    "        'Test_Accuracy': result['test_metrics']['accuracy'],\n",
    "        'Test_Precision': result['test_metrics']['precision'],\n",
    "        'Test_Recall': result['test_metrics']['recall'],\n",
    "        'Test_F1': result['test_metrics']['f1'],\n",
    "        'Training_Time': result['training_time']\n",
    "    }\n",
    "\n",
    "    # Add ROC-AUC if available\n",
    "    if 'roc_auc' in result['test_metrics']:\n",
    "        row['Test_ROC_AUC'] = result['test_metrics']['roc_auc']\n",
    "\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find best model\n",
    "best_model = comparison_df.loc[comparison_df['Test_Accuracy'].idxmax(), 'Model']\n",
    "best_accuracy = comparison_df['Test_Accuracy'].max()\n",
    "print(f\"\\nBest Model: {best_model} (Accuracy: {best_accuracy:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "x_pos = range(len(comparison_df))\n",
    "axes[0,0].bar(x_pos, comparison_df['Test_Accuracy'],\n",
    "              color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.8)\n",
    "axes[0,0].set_title('Test Accuracy by Model')\n",
    "axes[0,0].set_xlabel('Model')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].set_xticks(x_pos)\n",
    "axes[0,0].set_xticklabels(comparison_df['Model'])\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(comparison_df['Test_Accuracy']):\n",
    "    axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Multiple metrics comparison\n",
    "metrics = ['Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1']\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0,1].bar(x + i*width, comparison_df[metric], width,\n",
    "                  label=metric.replace('Test_', ''), alpha=0.8)\n",
    "\n",
    "axes[0,1].set_title('Multiple Metrics Comparison')\n",
    "axes[0,1].set_xlabel('Model')\n",
    "axes[0,1].set_ylabel('Score')\n",
    "axes[0,1].set_xticks(x + width * 1.5)\n",
    "axes[0,1].set_xticklabels(comparison_df['Model'])\n",
    "axes[0,1].legend()\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "\n",
    "# 3. Training time comparison\n",
    "axes[1,0].bar(x_pos, comparison_df['Training_Time'],\n",
    "              color=['gold', 'orange', 'red'], alpha=0.8)\n",
    "axes[1,0].set_title('Training Time by Model')\n",
    "axes[1,0].set_xlabel('Model')\n",
    "axes[1,0].set_ylabel('Training Time (seconds)')\n",
    "axes[1,0].set_xticks(x_pos)\n",
    "axes[1,0].set_xticklabels(comparison_df['Model'])\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(comparison_df['Training_Time']):\n",
    "    axes[1,0].text(i, v + 0.1, f'{v:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "# 4. Cross-validation scores with error bars\n",
    "axes[1,1].errorbar(x_pos, comparison_df['CV_Accuracy'],\n",
    "                   yerr=comparison_df['CV_Std'],\n",
    "                   fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "axes[1,1].set_title('Cross-Validation Accuracy (±1 Std)')\n",
    "axes[1,1].set_xlabel('Model')\n",
    "axes[1,1].set_ylabel('CV Accuracy')\n",
    "axes[1,1].set_xticks(x_pos)\n",
    "axes[1,1].set_xticklabels(comparison_df['Model'])\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b22e6",
   "metadata": {},
   "source": [
    "## 7. Detailed Analysis of Best Model\n",
    "\n",
    "Let's dive deeper into the performance of our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model results\n",
    "best_model_name = best_model.lower()\n",
    "best_result = results[best_model_name]\n",
    "best_trainer = best_result['trainer']\n",
    "\n",
    "print(f\"Detailed Analysis of {best_model} Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_trainer.model.predict(X_test)\n",
    "y_pred_proba = None\n",
    "\n",
    "if hasattr(best_trainer.model, 'predict_proba'):\n",
    "    y_pred_proba = best_trainer.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"   True Negatives:  {cm[0,0]:4d}    False Positives: {cm[0,1]:4d}\")\n",
    "print(f\"   False Negatives: {cm[1,0]:4d}    True Positives:  {cm[1,1]:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix and additional plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'{best_model} Model - Detailed Performance Analysis', fontsize=16)\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# 2. Feature Importance (if available)\n",
    "if hasattr(best_trainer.model, 'feature_importances_'):\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_trainer.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Plot top 10 features\n",
    "    top_features = feature_imp.head(10)\n",
    "    axes[1].barh(range(len(top_features)), top_features['importance'],\n",
    "                 color='lightgreen', alpha=0.8)\n",
    "    axes[1].set_yticks(range(len(top_features)))\n",
    "    axes[1].set_yticklabels(top_features['feature'])\n",
    "    axes[1].set_xlabel('Feature Importance')\n",
    "    axes[1].set_title('Top 10 Feature Importances')\n",
    "    axes[1].invert_yaxis()\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Feature importance\\nnot available for\\nthis model type',\n",
    "                 ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "    axes[1].set_title('Feature Importance')\n",
    "\n",
    "# 3. Prediction Probability Distribution (if available)\n",
    "if y_pred_proba is not None:\n",
    "    # Separate probabilities by actual class\n",
    "    prob_class_0 = y_pred_proba[y_test == 0]\n",
    "    prob_class_1 = y_pred_proba[y_test == 1]\n",
    "\n",
    "    axes[2].hist(prob_class_0, bins=30, alpha=0.7, label='Actual Class 0',\n",
    "                 color='skyblue', density=True)\n",
    "    axes[2].hist(prob_class_1, bins=30, alpha=0.7, label='Actual Class 1',\n",
    "                 color='lightcoral', density=True)\n",
    "    axes[2].axvline(x=0.5, color='black', linestyle='--', alpha=0.8, label='Decision Threshold')\n",
    "    axes[2].set_xlabel('Predicted Probability (Class 1)')\n",
    "    axes[2].set_ylabel('Density')\n",
    "    axes[2].set_title('Prediction Probability Distribution')\n",
    "    axes[2].legend()\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'Probability predictions\\nnot available for\\nthis model type',\n",
    "                 ha='center', va='center', transform=axes[2].transAxes, fontsize=12)\n",
    "    axes[2].set_title('Prediction Probabilities')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31978016",
   "metadata": {},
   "source": [
    "## 8. Model Persistence\n",
    "\n",
    "Let's save our best model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d15c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "print(f\"Saving {best_model} model...\")\n",
    "\n",
    "# Save model using our trainer\n",
    "model_path = best_trainer.save_model()\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Demonstrate loading the model\n",
    "print(f\"\\nTesting model loading...\")\n",
    "from src.models.trainer import get_trainer\n",
    "\n",
    "# Create new trainer and load model\n",
    "new_trainer = get_trainer(best_model_name)\n",
    "new_trainer.load_model(model_path)\n",
    "\n",
    "# Test prediction\n",
    "test_sample = X_test.iloc[:5]  # First 5 test samples\n",
    "original_pred = best_trainer.model.predict(test_sample)\n",
    "loaded_pred = new_trainer.model.predict(test_sample)\n",
    "\n",
    "print(f\"Original predictions: {original_pred}\")\n",
    "print(f\"Loaded predictions:   {loaded_pred}\")\n",
    "print(f\"Predictions match: {np.array_equal(original_pred, loaded_pred)}\")\n",
    "\n",
    "if np.array_equal(original_pred, loaded_pred):\n",
    "    print(\"Model saved and loaded successfully!\")\n",
    "else:\n",
    "    print(\"Model loading verification failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151dfbb2",
   "metadata": {},
   "source": [
    "## 9. Experiment Summary & Results\n",
    "\n",
    "Let's summarize our findings and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84960097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment summary\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: Synthetic classification dataset\")\n",
    "print(f\"Samples: {len(df):,} total, {len(X_train):,} train, {len(X_test):,} test\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Class balance: {df['target'].value_counts(normalize=True).round(3).tolist()}\")\n",
    "print(f\"Cross-validation: {config.get('training.cv_folds')}-fold stratified\")\n",
    "print(f\"Random seed: {config.get('environment.seed')}\")\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model}\")\n",
    "print(\"=\" * 60)\n",
    "best_metrics = best_result['test_metrics']\n",
    "print(f\"Test Accuracy:  {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"Test Recall:    {best_metrics['recall']:.4f}\")\n",
    "print(f\"Test F1-Score:  {best_metrics['f1']:.4f}\")\n",
    "if 'roc_auc' in best_metrics:\n",
    "    print(f\"Test ROC-AUC:   {best_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Training Time:  {best_result['training_time']:.2f} seconds\")\n",
    "\n",
    "cv_result = best_result['cv_results']\n",
    "print(f\"CV Accuracy:    {cv_result['cv_mean']:.4f} (±{cv_result['cv_std']:.4f})\")\n",
    "\n",
    "print(f\"\\nMODEL ARTIFACTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Saved model:     {model_path}\")\n",
    "print(f\"Configuration:   ../config/config.yaml\")\n",
    "print(f\"Experiment logs: ../logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836811fa",
   "metadata": {},
   "source": [
    "## 10. Next Steps & Recommendations\n",
    "\n",
    "Based on our analysis, here are the recommended next steps for improving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NEXT STEPS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Performance-based recommendations\n",
    "if best_metrics['accuracy'] > 0.95:\n",
    "    performance_level = \"Excellent\"\n",
    "    recommendations = [\n",
    "        \"Model performance is excellent!\",\n",
    "        \"Consider deploying to production\",\n",
    "        \"Monitor for concept drift over time\",\n",
    "        \"Collect more diverse real-world data\"\n",
    "    ]\n",
    "elif best_metrics['accuracy'] > 0.85:\n",
    "    performance_level = \"Good\"\n",
    "    recommendations = [\n",
    "        \"Model performance is good\",\n",
    "        \"Try hyperparameter optimization\",\n",
    "        \"Consider ensemble methods\",\n",
    "        \"Feature engineering improvements\"\n",
    "    ]\n",
    "else:\n",
    "    performance_level = \"Needs Improvement\"\n",
    "    recommendations = [\n",
    "        \"Model needs improvement\",\n",
    "        \"Collect more training data\",\n",
    "        \"Try different algorithms\",\n",
    "        \"Feature selection and engineering\",\n",
    "        \"Address class imbalance if present\"\n",
    "    ]\n",
    "\n",
    "print(f\"Performance Level: {performance_level}\")\n",
    "print(f\"Current Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(\"\\nIMMEDIATE ACTIONS:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(\"\\nTECHNICAL IMPROVEMENTS:\")\n",
    "tech_improvements = [\n",
    "    \"Implement hyperparameter optimization (Optuna)\",\n",
    "    \"Add feature selection pipeline\",\n",
    "    \"Set up automated model retraining\",\n",
    "    \"Implement model monitoring\",\n",
    "    \"Add A/B testing framework\",\n",
    "    \"Create model explanation tools (SHAP)\"\n",
    "]\n",
    "\n",
    "for i, improvement in enumerate(tech_improvements, 1):\n",
    "    print(f\"   {i}. {improvement}\")\n",
    "\n",
    "print(\"\\nLEARNING RESOURCES:\")\n",
    "resources = [\n",
    "    \"MLflow documentation for experiment tracking\",\n",
    "    \"Optuna for hyperparameter optimization\",\n",
    "    \"SHAP for model interpretability\",\n",
    "    \"MLOps best practices and tools\"\n",
    "]\n",
    "\n",
    "for i, resource in enumerate(resources, 1):\n",
    "    print(f\"   {i}. {resource}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be46a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete machine learning workflow using the **ML WSL Boilerplate** framework:\n",
    "\n",
    "- **Configuration Management** - Flexible, hierarchical configuration  \n",
    "- **Data Loading & Validation** - Custom DataLoader with validation  \n",
    "- **Exploratory Data Analysis** - Comprehensive data understanding  \n",
    "- **Model Training** - Multiple algorithms with cross-validation  \n",
    "- **Model Evaluation** - Detailed performance analysis  \n",
    "- **Model Persistence** - Save/load functionality  \n",
    "- **Experiment Tracking** - Structured logging and metrics  \n",
    "\n",
    "### Framework Benefits:\n",
    "\n",
    "- **Reproducible**: Fixed random seeds and versioned configurations\n",
    "- **Modular**: Separate components for data, models, and utilities\n",
    "- **Extensible**: Easy to add new models and features\n",
    "- **Production-Ready**: Logging, testing, and deployment tools\n",
    "- **Developer-Friendly**: Type hints, documentation, and VS Code integration\n",
    "\n",
    "### Ready for Production:\n",
    "\n",
    "Use `make train` to run the training pipeline from command line, or customize the configuration files for your specific use case!\n",
    "\n",
    "---\n",
    "\n",
    "*Happy Machine Learning!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
